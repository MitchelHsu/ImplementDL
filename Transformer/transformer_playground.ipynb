{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f4afcc8",
   "metadata": {},
   "source": [
    "# Implementing Transformer Architecture\n",
    "\n",
    "This is the implementation of the Transformers Architecture.\n",
    "        \n",
    "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\" width=\"30%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "774e64d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your using \"cuda\" as your training device\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import numpy as np\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "# Use Apple Silicon\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "# Use NVIDIA GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "print(f'Your using \"{device}\" as your training device')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ebd818",
   "metadata": {},
   "source": [
    "## Embedding Layer\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1200/1*sAJdxEsDjsPMioHyzlN3_A.png\" width=\"80%\" />\n",
    "<!-- <img src=\"https://miro.medium.com/max/906/1*B-VR6R5vJl3Y7jbMNf5Fpw.png\" width=\"80%\" /> -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfafe2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Embedding):\n",
    "    def __init__(self, vocab_size, embed_dim=512):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da119e7",
   "metadata": {},
   "source": [
    "## Scaled Dot-Product Attention\n",
    "<img src=\"https://velog.velcdn.com/images%2Fcha-suyeon%2Fpost%2Fba830026-6d8f-4e77-b288-f75dd3a51457%2Fimage.png\" />\n",
    "\n",
    "## Multi-Head Attention\n",
    "<img src=\"https://www.researchgate.net/publication/333078019/figure/fig1/AS:758304078839808@1557805189409/left-Scaled-Dot-Product-Attention-right-Multi-Head-Attention.png\" width=\"60%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131ec63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int):\n",
    "        assert d_model % num_heads == 0, \"The embedding dimension should be divsible by the number of heads\"\n",
    "\n",
    "        # Initial weights for Key, Query, Value, and\n",
    "        # Shape of the weights should all be (batch_size, d_model, d_model)\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        self.W_O = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, K: torch.Tensor, Q: torch.Tensor, V: torch.Tensor, mask: torch.Tensor=None) -> torch.Tensor:\n",
    "        '''\n",
    "        This is the implementation of Scaled Dot-Product Attention\n",
    "        \n",
    "        Dimension of K, Q, V: (batch_size, num_heads, seq_len, embedding_dim)\n",
    "        '''\n",
    "        # QK_t / √d_model, the scaled-dot value\n",
    "        # The dim of both Q and V is 32x8x10x64, the dim of V_t will be 32x8x64x10\n",
    "        # So the dim of attention_score will be 32x8x10x10\n",
    "        attention_score = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_model)\n",
    "        \n",
    "        # Apply mask\n",
    "        if mask is not None:\n",
    "            attention_score = attention_score.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Softmax(QK_t / √d_model)\n",
    "        attention_probs = torch.softmax(attention_score, dim=-1)\n",
    "        \n",
    "        # Softmax(QK_t / √d_model) x V\n",
    "        # attention_score(32x8x10x10) x V(32x8x10x64) -> output dim will be 32x8x10x64\n",
    "        # Which input dim == output dim\n",
    "        output = torch.matmul(attention_probs, V)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def split_heads(self, x: torch.Tensor):\n",
    "        pass\n",
    "\n",
    "    def fuse_heads(self, x: torch.Tensor):\n",
    "        pass\n",
    "\n",
    "    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, mask: torch.Tensor=None) -> torch.Tensor:\n",
    "        Q = self.split_heads(self.W_Q(Q))\n",
    "        K = self.split_heads(self.W_K(K))\n",
    "        V = self.split_heads(self.W_V(V))\n",
    "\n",
    "        attention = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_O(self.fuse_heads(attention))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ceb81b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 8, 10, 10])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = torch.tensor(np.random.rand(32, 8, 10, 64))\n",
    "K = torch.tensor(np.random.rand(32, 8, 10, 64))\n",
    "\n",
    "torch.matmul(Q, K.transpose(-2, -1)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4800c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dc3844",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
